<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Joris Paret</title>
    <link>https://jorisparet.github.io/tag/deep-learning/</link>
      <atom:link href="https://jorisparet.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 07 Sep 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jorisparet.github.io/media/icon_huf5b7e9305d8ca00c36bcf7194f7e4936_69678_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://jorisparet.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>First official release of hamoco</title>
      <link>https://jorisparet.github.io/post/2022-09-07_hamoco-first-release/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://jorisparet.github.io/post/2022-09-07_hamoco-first-release/</guid>
      <description>&lt;p&gt;&lt;strong&gt;hamoco&lt;/strong&gt; (&lt;em&gt;handy mouse controller&lt;/em&gt;) allows you to take control of your mouse by using hand gestures that are captured in real time by your webcam. It relies on &lt;a href=&#34;https://google.github.io/mediapipe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MediaPipe&lt;/a&gt; to track hands, and the nature of the different hand poses are predicted by a small neural network built with &lt;a href=&#34;https://www.tensorflow.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow&lt;/a&gt;. Basically, I thought that it might be fun to try and replicate the famous scene from the movie &lt;a href=&#34;https://en.wikipedia.org/wiki/Minority_Report_%28film%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minority Report&lt;/a&gt; (spoiler: it&amp;rsquo;s cooler when Tom Cruise does it).&lt;/p&gt;
&lt;p&gt;You can perform all the basic mouse actions: &lt;em&gt;motion&lt;/em&gt;, &lt;em&gt;left/right click&lt;/em&gt;, &lt;em&gt;vertical scrolling&lt;/em&gt; and &lt;em&gt;drag &amp;amp; drop&lt;/em&gt;. There are many options to adjust the experience to your liking (&lt;em&gt;e.g.&lt;/em&gt; sensitivity, motion smoothing) and even an automated pipeline to record your own data and train a custom neural network tailored to your needs.&lt;/p&gt;
&lt;p&gt;The code is now available on &lt;a href=&#34;https://pypi.org/project/hamoco/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyPI&lt;/a&gt; in version 1.0.1, and you can also check out the page of the project on &lt;a href=&#34;https://github.com/jorisparet/hamoco&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hamoco</title>
      <link>https://jorisparet.github.io/project/hamoco/</link>
      <pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate>
      <guid>https://jorisparet.github.io/project/hamoco/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://jorisparet.github.io/project/hamoco/logo.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;hamoco&#34;&gt;Hamoco&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;hamoco&lt;/strong&gt; (&lt;em&gt;handy mouse controller&lt;/em&gt;) is a python application that allows you to control your mouse from your webcam using various hand gestures. You have a laptop equipped with a webcam? Well, good news, that&amp;rsquo;s all you need to feel like Tom Cruise in &lt;a href=&#34;https://en.wikipedia.org/wiki/Minority_Report_%28film%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minority Report&lt;/a&gt;! Kind of.&lt;/p&gt;
&lt;h3 id=&#34;demonstration&#34;&gt;Demonstration&lt;/h3&gt;
&lt;p&gt;In the example below, the hand is used to move the pointer, open a file by double-clicking on it, scroll through it, select a paragraph and cut it. The file is then dragged and dropped into a folder.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/jorisparet/hamoco/main/images/demo.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h3&gt;
&lt;p&gt;By using the power of &lt;a href=&#34;https://pypi.org/project/PyAutoGUI/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyAutoGUI&lt;/a&gt; to control the mouse, &lt;a href=&#34;https://pypi.org/project/opencv-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenCV&lt;/a&gt; to process the video feed, and &lt;a href=&#34;https://google.github.io/mediapipe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MediaPipe&lt;/a&gt; to track hands, &lt;strong&gt;hamoco&lt;/strong&gt; predicts the nature of a hand pose in real-time thanks to a neural network built with &lt;a href=&#34;https://keras.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keras&lt;/a&gt; and uses it to perform various kinds of mouse pointer actions.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; From &lt;a href=&#34;https://pypi.org/project/hamoco/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyPI&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install hamoco
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; From the &lt;a href=&#34;https://github.com/jorisparet/hamoco&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code repository&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/jorisparet/hamoco
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cd hamoco
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install -r requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install .
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;p&gt;The installation copies three scripts in the default script folder of &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;hamoco-run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-data&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-train&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;linux&#34;&gt;Linux&lt;/h4&gt;
&lt;p&gt;The default folder should be under &lt;code&gt;/home/&amp;lt;user&amp;gt;/.local/bin/&lt;/code&gt;. Make sure this location (or the correct one, if different) is included in your &lt;code&gt;$PATH&lt;/code&gt; environment variable to be able to run the scripts from the console. If not, type the following command &lt;code&gt;export PATH=$PATH:/path/to/hamoco/scripts/&lt;/code&gt; in the console or add it your &lt;code&gt;.bashrc&lt;/code&gt; file.&lt;/p&gt;
&lt;h4 id=&#34;windows&#34;&gt;Windows&lt;/h4&gt;
&lt;p&gt;The default folder should be under &lt;code&gt;C:\Users\&amp;lt;user&amp;gt;\AppData\Local\Programs\Python\&amp;lt;python_version&amp;gt;\Scripts\&lt;/code&gt;. Make sure this location (or the correct one, if different) is included in your &lt;code&gt;$PATH&lt;/code&gt; environment variable to be able to run the scripts from the console. If not, type the following command &lt;code&gt;set PATH=%PATH%;C:\path\to\hamoco\scripts\&lt;/code&gt; in the console, or select &lt;code&gt;Edit the system environment variables&lt;/code&gt; in the search bar, click &lt;code&gt;Environment Variablesâ€¦&lt;/code&gt;, click &lt;code&gt;PATH&lt;/code&gt;, click &lt;code&gt;Edit...&lt;/code&gt; and add the correct path to the scripts.&lt;/p&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.org/project/PyAutoGUI/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyAutoGUI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.org/project/numpy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NumPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pypi.org/project/opencv-python/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://google.github.io/mediapipe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MediaPipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;Quick start&lt;/h2&gt;
&lt;h3 id=&#34;running-the-scripts&#34;&gt;Running the scripts&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;hamoco&lt;/strong&gt; is composed of three executable scripts: &lt;em&gt;&lt;a href=&#34;#hamoco-run&#34;&gt;hamoco-run&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href=&#34;#hamoco-data&#34;&gt;hamoco-data&lt;/a&gt;&lt;/em&gt;, and &lt;em&gt;&lt;a href=&#34;#hamoco-train&#34;&gt;hamoco-train&lt;/a&gt;&lt;/em&gt;, that are listed below. Run these scripts directly from the console, &lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;hamoco-run --sensitivity 0.5 --show&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;hamoco-run&#34;&gt;hamoco-run&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;hamoco-run&lt;/em&gt; is the &lt;strong&gt;main application&lt;/strong&gt;. It activates the webcam and allows to use hand gestures to take control of the mouse pointer. Several basic actions can then be performed, such as &lt;em&gt;left click&lt;/em&gt;, &lt;em&gt;right click&lt;/em&gt;, &lt;em&gt;drag and drop&lt;/em&gt; and &lt;em&gt;scrolling&lt;/em&gt;. Note that it requires &lt;strong&gt;a bit of practice&lt;/strong&gt; before getting comfortable with the controls. Various settings can be adjusted to customize the hand controller to your liking, such as the global sensivitity, parameters for motion smoothing and much more. Type &lt;code&gt;hamoco-run --help&lt;/code&gt; for more information on the available options.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;hamoco-run --sensitivity 0.4 --scrolling_threshold 0.2&lt;/code&gt; : adapts the sensitivity and sets a custom threshold value to trigger scrolling motions.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-run --min_cutoff_filter 0.05 --show&lt;/code&gt; : sets a custom value for the cutoff frequency used for motion smoothing and opens a window that shows the processed video feed in real-time.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-run --scrolling_speed 20&lt;/code&gt; : sets a custom value for the scrolling speed. Note that for a given value, results may differ significantly depending on the operating system.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-run --margin 0.2 --stop_sequence THUMB_SIDE CLOSE INDEX_MIDDLE_UP&lt;/code&gt; : adapts the size of the detection margin (indicated by the dark frame in the preview windows using &lt;code&gt;--show&lt;/code&gt;), and changes the sequence of consecutive poses to stop the application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration files with default values for the control parameters can be found in the installation folder, under &lt;code&gt;hamoco/config/&lt;/code&gt;. Simply edit the file that corresponds to your operating system (&lt;code&gt;posix.json&lt;/code&gt; for &lt;strong&gt;Linux&lt;/strong&gt; and &lt;code&gt;nt.json&lt;/code&gt; for &lt;strong&gt;Windows&lt;/strong&gt;) to save your settings permanently, and hence avoid specifying the parameters by hand in the console.&lt;/p&gt;
&lt;h4 id=&#34;hand-poses--mouse-actions&#34;&gt;Hand poses &amp;amp; Mouse actions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;OPEN&lt;/code&gt; : the pointer is free and follows the center of the palm (indicated by the white square) ;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CLOSE&lt;/code&gt; : the pointer stops all actions. The hand can be moved anywhere in the frame without moving the pointer. This is used to reset the origin of motion (see the &lt;em&gt;nota bene&lt;/em&gt; below) ;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INDEX_UP&lt;/code&gt; : performs a left-click at the current pointer location. Execute twice rapidly for a double-click ;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PINKY_UP&lt;/code&gt; : performs a right click at the current pointer location ;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INDEX_MIDDLE_UP&lt;/code&gt; : holds the left mouse button down and moves the pointer by following the center of the palm. This is used for selection and drag &amp;amp; drop ;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;THUMB_SIDE&lt;/code&gt; : enables vertical scrolling using the first triggering location as origin. Scrolling up or down is done by moving the hand up or down relative to the origin while keeping the same hand pose ;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;N.B.&lt;/strong&gt; note that, much like a real mouse, the recorded motion of the pointer is &lt;em&gt;relative&lt;/em&gt; to its previous position. When your mouse reaches the edge of your mouse pad, you simply lift it and land it back somewhere on the pad to start moving again. Similarly, if your hand reaches the edge of the frame, the pointer will stop moving: simply close your fist and move it back into the frame to reset the origin of motion (exactly like when lifting and moving a real mouse).&lt;/p&gt;
&lt;p&gt;The various hand poses are illustrated below:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/jorisparet/hamoco/main/images/hand_poses.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;exiting-the-application&#34;&gt;Exiting the application:&lt;/h4&gt;
&lt;p&gt;There are two ways to exit the application:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the preview mode (&lt;code&gt;--show&lt;/code&gt; option enabled), simply click on the preview window and press &lt;code&gt;ESC&lt;/code&gt; ;&lt;/li&gt;
&lt;li&gt;Execute a predetermined sequence of consecutive hand poses. The default sequence can be found in the help message (&lt;code&gt;hamoco-run --help&lt;/code&gt;). A new sequence can be specified with the &lt;code&gt;--stop_sequence&lt;/code&gt; option followed by the consecutive hand poses, or it can simply be changed in the &lt;code&gt;.json&lt;/code&gt; configuration file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;hamoco-data&#34;&gt;hamoco-data&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;hamoco-data&lt;/em&gt; activates the webcam and allows to record your own labeled data for hand poses in order to train a custom neural-network-based classification model for the main application. This model can then be used in place of the one provided by default and will be more performant, as it will be trained on your personal and natural hand poses (see &lt;em&gt;&lt;a href=&#34;#hamoco-train&#34;&gt;hamoco-train&lt;/a&gt;&lt;/em&gt;). Type &lt;code&gt;hamoco-data --help&lt;/code&gt; for more information on the available options.&lt;/p&gt;
&lt;p&gt;This application requires two arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pose&lt;/code&gt;: a string that indicates the type of hand pose you intend to record. It should be one of: &lt;code&gt;OPEN&lt;/code&gt;, &lt;code&gt;CLOSE&lt;/code&gt;, &lt;code&gt;INDEX_UP&lt;/code&gt;, &lt;code&gt;PINKY_UP&lt;/code&gt;, &lt;code&gt;THUMB_SIDE&lt;/code&gt;, &lt;code&gt;INDEX_MIDDLE_UP&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;path_to_data&lt;/code&gt;: path to the folder inside of which you want the recorded data to be saved.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;hamoco-data OPEN data/ --delay 1.0&lt;/code&gt; : starts the recording for the &lt;code&gt;OPEN&lt;/code&gt; hand pose, stores the resulting data in the &lt;code&gt;data&lt;/code&gt; folder (provided it exists!), and takes a new snapshot every second.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-data INDEX_UP data/ --delay 0.25 --images&lt;/code&gt; : starts the recording for the &lt;code&gt;INDEX_UP&lt;/code&gt; hand pose, stores the resulting data in the &lt;code&gt;data&lt;/code&gt; folder, takes a new snapshot every 0.25s, and saves the images (in addition to the numeric data file used for training the model). Saving images can be useful if you want to manually check if your hand was in a correct position when its numerical data was recorded, and hence keep or remove specific data files accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-data CLOSE data/ --reset --stop_after 200&lt;/code&gt; : starts the recording of the &lt;code&gt;CLOSE&lt;/code&gt; hand pose, stores the resulting data in the &lt;code&gt;data&lt;/code&gt; folder, deletes every previously recorded file for this hand pose, and automatically stop the recording after taking 200 snapshots.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hamoco-train&#34;&gt;hamoco-train&lt;/h3&gt;
&lt;p&gt;Provided a path to a directory with compatible data, &lt;em&gt;hamoco-train&lt;/em&gt; trains a customizable NN-based classification model to predict a hand pose. This classification model can then be used in the main application in place of the one provided by default. Type &lt;code&gt;hamoco-train --help&lt;/code&gt; for more information on the available options.&lt;/p&gt;
&lt;p&gt;This application requires two arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;path_to_model&lt;/code&gt; : path to save the newly trained model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;path_to_data&lt;/code&gt; : path to the data folder to use to train the model (see &lt;em&gt;&lt;a href=&#34;#hamoco-data&#34;&gt;hamoco-data&lt;/a&gt;&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;hamoco-train my_custom_model.h5 data/ --hiden_layers 50 25 --epochs 20&lt;/code&gt; : trains and save a model named &lt;code&gt;my_custom_model.h5&lt;/code&gt; that contains two hidden layers (with dimensions 50 and 25 respectively) over 20 epochs, by using the compatible data in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hamoco-train my_custom_model.h5 data/ --epochs 10 --learning_rate 0.1&lt;/code&gt; : trains and save a model named &lt;code&gt;my_custom_model.h5&lt;/code&gt; with default dimensions over 20 epochs and with a learning rate of 0.1, by using the compatible data in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your model can then be used in the main application with the &lt;code&gt;--model&lt;/code&gt; flag of &lt;em&gt;&lt;a href=&#34;#hamoco-run&#34;&gt;hamoco-run&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;hamoco-run --model &amp;lt;path_to_your_model&amp;gt;&lt;/code&gt; , or you can change the &lt;code&gt;.json&lt;/code&gt; configuration file to point to it.&lt;/p&gt;
&lt;h2 id=&#34;author&#34;&gt;Author&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.jorisparet.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joris Paret&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
